## 变更摘要
- 数据库确定为 Supabase（免费层，Postgres + Storage）。
- 录音可能长达 2 小时：后端采用流式处理与大文件支持；必要时引入上传到 Supabase Storage 的预签名直传。
- Whisper：需自动检测语言与分段；说明：Whisper 不提供“说话人身份/分离”（speaker diarization），如需对话脚本（按说话人分配），需额外集成说话人分离模型（后续阶段接入）。

## 项目概览
- 目标（第1步）：iPhone 录音 → 快捷指令携定位/时间上传 → 后端调用 Whisper → 返回简洁 HTML 展示：转录文本、自动检测语言、分段、时间/位置。
- 路线：本地开发 → GitHub 托管 → Render 部署后端；后续接 Supabase 并实现前端。

## 技术选型与约束
- 后端：Node.js + Express（Render 部署）
- 转录：OpenAI Whisper（`whisper-1`），`response_format: verbose_json` 获取语言与分段（含每段起止时间）
- 数据库：Supabase（Postgres）与 Supabase Storage（大音频文件可选保存）
- 前端：Next.js（Vercel）于第4步实现
- 大文件支持：
  - 直接上传：后端使用 `multer` 或原生流处理，避免一次性加载内存
  - 备选路径：后端发放 Supabase Storage 预签名 URL，快捷指令直传音频；后端以 URL 拉取音频并转录（用于超大或网络不稳场景）

## 接口与数据约定（第1步范围）
- 上传方式：`multipart/form-data`
- 字段：
  - `file`: `.m4a` 音频文件（最长目标支持约 2 小时）
  - `started_at`: ISO-8601 开始时间
  - `duration_seconds`: 时长（秒）
  - `latitude` / `longitude`: 坐标
  - 可选：`accuracy`、`timezone`、`device_model`
- 接口：`POST /api/upload-audio`
  - 行为：接收文件（流式）→ Whisper 转录（自动语言 + 分段）→ 返回 HTML 展示
  - 超大文件备选：`POST /api/upload-audio-url`（传 `audio_url` 为 Supabase Storage 地址）

## Whisper 调用细则
- 模型：`whisper-1`
- 语言检测：自动（无需指定 `language`）
- 输出：`verbose_json`（包含 `text`、`language`、`duration`、`segments[{start,end,text}]`）
- 说话人分离：Whisper 不支持；后续可集成 pyannote（开源）、或第三方服务实现 diarization，将分段映射到说话人标签（如 `Speaker A/B/...`）

## 后端结果页（第1步）
- 展示：
  - 转录全文
  - 语言（Whisper 检测）
  - 录音开始时间、时长、坐标
  - 分段列表（每段时间戳与文本）
- 错误页：清晰错误信息与状态码

## iPhone 快捷指令流程（第1步）
- 步骤：
  - 录制音频（导出 `.m4a`）
  - 获取当前位置（纬度/经度/精度）
  - 获取开始时间（ISO 格式）
  - 获取媒体详细信息（音频时长）
  - 获取 URL 的内容（POST 表单，字段与文件如上）
  - 显示返回的 HTML（结果页）
- 备选：
  - 调用后端 `GET /api/presign` 获取 Supabase Storage 预签名上传地址
  - 直传音频到 Storage → 调用 `POST /api/upload-audio-url` 仅传 URL 与元数据

## 环境与安全
- Render 环境变量：`OPENAI_API_KEY`、（后续）`SUPABASE_URL`、`SUPABASE_SERVICE_ROLE` 或仅用于存储的 `SUPABASE_STORAGE_KEY`
- 上传限制与校验：文件类型与最大大小、必填字段校验、请求超时/重试策略

## 本地仓库结构（将于第1步创建）
- `backend/`：Express 服务
  - 路由：`POST /api/upload-audio`，`POST /api/upload-audio-url`（备选），`GET /api/presign`（备选）
  - 服务：Whisper 客户端、HTML 模板
- 后续：`db/`（Supabase 客户端与表结构）、`frontend/`（Next.js）

## 验证与交付
- 提供 `curl` 测试（本地与 Render 环境）
- 用 5–10 分钟样例音频验证分段与语言检测；对 2 小时音频进行稳定性测试与超时设置（Render 与 OpenAI API 超时需调优）

## 后续阶段（不在本次实施范围）
- 第2步：DeepSeek 总结/标题/多话题提取，并将分段作为上下文
- 第2.5步（新增）：说话人分离（pyannote/第三方）→ 对话脚本化（Speaker A/B）
- 第3步：接入 Supabase（Postgres + Storage），存储转录、摘要、标题、话题、位置/时间、音频 URL
- 第4步：Vercel 前端时间轴展示与检索

请确认是否按以上更新方案执行第1步实现